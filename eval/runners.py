"""LLM provider runners for IB-bench evaluation pipeline."""

import os
import time
from dataclasses import dataclass
from pathlib import Path
from typing import Any, cast

from helpers import Task, extract_json, retry_on_rate_limit


def _is_content_filter_error(error_str: str) -> bool:
    """Check if an error message indicates a content filter block."""
    error_lower = error_str.lower()
    return (
        (
            "content" in error_lower
            and (
                "policy" in error_lower
                or "blocked" in error_lower
                or "safety" in error_lower
            )
        )
        or "invalid_prompt" in error_lower
        or "invalid prompt" in error_lower
        or "flagged" in error_lower
        or "usage policy" in error_lower
        or "content_policy" in error_lower
        or "moderation" in error_lower
    )


def _read_file_content(file_obj) -> bytes:
    """Read content from a file object, handling both stream and bytes."""
    if hasattr(file_obj, "read"):
        return file_obj.read()
    return file_obj


@dataclass
class OutputFile:
    """A file generated by the LLM (e.g., from code execution)."""

    filename: str
    content: bytes
    mime_type: str = "application/octet-stream"


@dataclass
class LLMResponse:
    """Standardized response from any LLM provider."""

    raw_text: str
    parsed_json: dict[str, Any] | None
    model: str
    input_tokens: int
    output_tokens: int
    latency_ms: float
    stop_reason: str = "unknown"
    output_files: list[OutputFile] | None = None


class AnthropicRunner:
    """Run tasks against Anthropic Claude models with code execution for Excel files."""

    def __init__(self, api_key: str | None = None, model: str | None = None):
        if not model:
            raise ValueError("model is required for AnthropicRunner")
        self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError("ANTHROPIC_API_KEY not set")
        self.model = model
        self._client = None

    @property
    def client(self):
        if self._client is None:
            import anthropic

            self._client = anthropic.Anthropic(api_key=self.api_key)
        return self._client

    @retry_on_rate_limit(max_retries=3, initial_wait=60)
    def run(self, task: Task, input_files: list[Path] | None = None) -> LLMResponse:
        """Execute a task against Claude with file upload via Files API."""
        files = input_files or []
        if files:
            return self._run_with_files(task, files)
        else:
            return self._run_text_only(task)

    def _run_text_only(self, task: Task) -> LLMResponse:
        """Run task with text prompt only (no Excel file)."""
        start = time.time()
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=16384,
                temperature=0,
                messages=[{"role": "user", "content": task.prompt}],
            )
        except Exception as e:
            latency_ms = (time.time() - start) * 1000
            if _is_content_filter_error(str(e)):
                print("  BLOCKED: Content filter triggered")
                return LLMResponse(
                    raw_text="",
                    parsed_json=None,
                    model=self.model,
                    input_tokens=0,
                    output_tokens=0,
                    latency_ms=latency_ms,
                    stop_reason="content_filter",
                    output_files=None,
                )
            raise

        latency_ms = (time.time() - start) * 1000

        first_block = response.content[0]
        raw_text: str = getattr(first_block, "text", "")
        parsed_json = extract_json(raw_text)
        stop_reason = response.stop_reason or "unknown"

        if stop_reason == "max_tokens":
            print("  WARNING: Output truncated (hit max_tokens limit)")

        return LLMResponse(
            raw_text=raw_text,
            parsed_json=parsed_json,
            model=self.model,
            input_tokens=response.usage.input_tokens,
            output_tokens=response.usage.output_tokens,
            latency_ms=latency_ms,
            stop_reason=stop_reason,
            output_files=None,
        )

    def _run_with_files(self, task: Task, input_files: list[Path]) -> LLMResponse:
        """Run with file upload - uploads files via Files API and uses code execution."""
        file_ids = []
        for input_file in input_files:
            print(f"Uploading {input_file.name} to Files API...")
            with open(input_file, "rb") as f:
                file_obj = self.client.beta.files.upload(file=f)
                file_ids.append(file_obj.id)

        content: list[Any] = []
        for file_id in file_ids:
            content.append({"type": "container_upload", "file_id": file_id})
        content.append({"type": "text", "text": task.prompt})

        start = time.time()
        content_filter_triggered = False
        messages: list[Any] = [{"role": "user", "content": content}]
        all_content_blocks: list[Any] = []
        total_input_tokens = 0
        total_output_tokens = 0
        final_stop_reason = "unknown"
        max_pause_continuations = 10
        container_id = None

        try:
            for continuation in range(max_pause_continuations + 1):
                try:
                    response = self.client.beta.messages.create(
                        model=self.model,
                        betas=["code-execution-2025-08-25", "files-api-2025-04-14"],
                        max_tokens=16384,
                        temperature=0,
                        messages=messages,
                        tools=[
                            {
                                "type": "code_execution_20250825",
                                "name": "code_execution",
                            }
                        ],
                    )
                except Exception as e:
                    if _is_content_filter_error(str(e)):
                        print("  BLOCKED: Content filter triggered")
                        content_filter_triggered = True
                        break
                    raise

                all_content_blocks.extend(response.content)
                total_input_tokens += response.usage.input_tokens
                total_output_tokens += response.usage.output_tokens
                final_stop_reason = response.stop_reason or "unknown"

                resp_container = getattr(response, "container", None)
                if resp_container:
                    container_id = getattr(resp_container, "id", None) or container_id

                if final_stop_reason != "pause_turn":
                    break

                print(f"  Model paused (continuation {continuation + 1}), resuming...")
                messages.append({"role": "assistant", "content": response.content})

            if final_stop_reason == "pause_turn":
                print(
                    f"  WARNING: Model still paused after {max_pause_continuations} continuations"
                )

        finally:
            for file_id in file_ids:
                try:
                    self.client.beta.files.delete(file_id)
                except Exception as e:
                    print(f"  Warning: Failed to delete file {file_id}: {e}")

        latency_ms = (time.time() - start) * 1000

        if content_filter_triggered:
            return LLMResponse(
                raw_text="",
                parsed_json=None,
                model=self.model,
                input_tokens=total_input_tokens,
                output_tokens=total_output_tokens,
                latency_ms=latency_ms,
                stop_reason="content_filter",
                output_files=None,
            )

        raw_text = ""
        output_files: list[OutputFile] = []
        extracted_file_ids: list[str] = []

        block_types = [getattr(b, "type", "unknown") for b in all_content_blocks]

        for block in all_content_blocks:
            block_type = getattr(block, "type", None)

            if block_type == "text":
                block_text = getattr(block, "text", None)
                if block_text:
                    raw_text += block_text + "\n"

            elif block_type == "bash_code_execution_tool_result":
                block_content = getattr(block, "content", None)
                if block_content:
                    content_type = getattr(block_content, "type", None)
                    if content_type == "bash_code_execution_result":
                        stdout = getattr(block_content, "stdout", None)
                        if stdout:
                            raw_text += stdout + "\n"
                        inner_content = getattr(block_content, "content", []) or []
                        for item in inner_content:
                            file_id = getattr(item, "file_id", None)
                            if file_id:
                                extracted_file_ids.append(file_id)

            elif block_type == "text_editor_code_execution_tool_result":
                block_content = getattr(block, "content", None)
                if block_content:
                    content_text = getattr(block_content, "content", None)
                    if content_text and isinstance(content_text, str):
                        raw_text += content_text + "\n"

            elif block_type == "code_execution_result":
                container_id = getattr(block, "container_id", None) or container_id
                block_content = getattr(block, "content", None)
                if block_content:
                    for item in block_content:
                        stdout = getattr(item, "stdout", None)
                        item_text = getattr(item, "text", None)
                        if stdout:
                            raw_text += stdout + "\n"
                        elif item_text:
                            raw_text += item_text + "\n"

        for file_id in extracted_file_ids:
            try:
                file_metadata = self.client.beta.files.retrieve_metadata(file_id)
                print(f"  Downloading output file: {file_metadata.filename}")
                file_content = self.client.beta.files.download(file_id)
                output_files.append(
                    OutputFile(
                        filename=file_metadata.filename,
                        content=cast(bytes, _read_file_content(file_content)),
                        mime_type=getattr(
                            file_metadata, "mime_type", "application/octet-stream"
                        ),
                    )
                )
            except Exception as e:
                print(f"  Warning: Failed to download file {file_id}: {e}")

        if not output_files and container_id:
            try:
                container_files = self.client.beta.files.list(
                    container_id=container_id  # type: ignore[call-arg]
                )
                for cf in container_files.data:
                    print(f"  Downloading output file: {cf.filename}")
                    file_content = self.client.beta.files.download(cf.id)
                    output_files.append(
                        OutputFile(
                            filename=cf.filename,
                            content=cast(bytes, _read_file_content(file_content)),
                            mime_type=getattr(
                                cf, "mime_type", "application/octet-stream"
                            ),
                        )
                    )
            except Exception as e:
                print(f"  Warning: Failed to retrieve container files: {e}")

        parsed_json = extract_json(raw_text)

        if final_stop_reason == "max_tokens":
            print("  WARNING: Output truncated (hit max_tokens limit)")

        return LLMResponse(
            raw_text=raw_text,
            parsed_json=parsed_json,
            model=self.model,
            input_tokens=total_input_tokens,
            output_tokens=total_output_tokens,
            latency_ms=latency_ms,
            stop_reason=final_stop_reason,
            output_files=output_files if output_files else None,
        )


class OpenAIRunner:
    """Run tasks against OpenAI models using Responses API."""

    def __init__(self, api_key: str | None = None, model: str | None = None):
        if not model:
            raise ValueError("model is required for OpenAIRunner")
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY not set")
        self.model = model
        self._client = None

    @property
    def client(self):
        if self._client is None:
            import openai

            self._client = openai.OpenAI(api_key=self.api_key)
        return self._client

    def _upload_file(self, path: Path) -> str:
        """Upload file to OpenAI Files API for use with Responses API."""
        with open(path, "rb") as f:
            file = self.client.files.create(file=f, purpose="user_data")
        return file.id

    # OpenAI's file_search tool requires files to be indexed in a vector store
    # before querying. We create a temp store, add files, then poll until
    # indexing completes (async on their end).
    def _create_vector_store(self, file_ids: list[str]) -> str:
        """Create a vector store with uploaded files for file_search."""
        vs = self.client.vector_stores.create(name="ib-bench-temp")
        for fid in file_ids:
            self.client.vector_stores.files.create(vector_store_id=vs.id, file_id=fid)
        print("Waiting for vector store indexing...")
        while True:
            vs_status = self.client.vector_stores.retrieve(vs.id)
            if vs_status.file_counts.completed == len(file_ids):
                break
            if vs_status.file_counts.failed > 0:
                print(
                    f"  Warning: {vs_status.file_counts.failed} file(s) failed to index"
                )
                break
            time.sleep(1)
        return vs.id

    @retry_on_rate_limit(max_retries=3, initial_wait=60)
    def run(self, task: Task, input_files: list[Path] | None = None) -> LLMResponse:
        """Execute a task using Responses API with tools."""
        import base64

        start = time.time()
        files = input_files or []
        tools: list[dict] = [{"type": "web_search"}]
        vector_store_id = None
        uploaded_file_ids = []

        pdf_files = [f for f in files if f.suffix.lower() == ".pdf"]
        code_files = [f for f in files if f.suffix.lower() in [".xlsx", ".xls", ".csv"]]
        image_files = [
            f for f in files if f.suffix.lower() in [".png", ".jpg", ".jpeg"]
        ]

        if pdf_files:
            print(f"Uploading {len(pdf_files)} PDF(s) for file search...")
            pdf_file_ids = []
            for pdf in pdf_files:
                fid = self._upload_file(pdf)
                pdf_file_ids.append(fid)
                uploaded_file_ids.append(fid)
            vector_store_id = self._create_vector_store(pdf_file_ids)
            tools.append(
                {
                    "type": "file_search",
                    "vector_store_ids": [vector_store_id],
                }
            )

        code_file_ids = []
        if code_files:
            print(f"Uploading {len(code_files)} file(s) for code interpreter...")
            for cf in code_files:
                fid = self._upload_file(cf)
                code_file_ids.append(fid)
                uploaded_file_ids.append(fid)
            tools.append(
                {
                    "type": "code_interpreter",
                    "container": {"type": "auto", "file_ids": code_file_ids},
                }
            )

        input_content = []

        for img in image_files:
            print(f"Encoding {img.name} as base64...")
            with open(img, "rb") as f:
                img_data = base64.b64encode(f.read()).decode("utf-8")
            ext = img.suffix.lower().replace(".", "")
            mime = f"image/{ext}" if ext != "jpg" else "image/jpeg"
            input_content.append(
                {
                    "type": "input_image",
                    "image_url": {"url": f"data:{mime};base64,{img_data}"},
                }
            )

        input_content.append(
            {
                "type": "input_text",
                "text": task.prompt,
            }
        )

        if len(input_content) == 1:
            api_input = task.prompt
        else:
            api_input = [{"role": "user", "content": input_content}]

        print("Running Responses API...")
        content_filter_triggered = False
        try:
            response = self.client.responses.create(
                model=self.model,
                input=api_input,  # type: ignore[arg-type]
                tools=tools if tools else None,  # type: ignore[arg-type]
                temperature=0,
                max_output_tokens=16384,
            )
        except Exception as e:
            if _is_content_filter_error(str(e)):
                print("  BLOCKED: Content filter triggered")
                content_filter_triggered = True
                response = None
            else:
                raise
        finally:
            if vector_store_id:
                try:
                    self.client.vector_stores.delete(vector_store_id)
                except Exception as e:
                    print(f"  Warning: Failed to delete vector store: {e}")

            for fid in uploaded_file_ids:
                try:
                    self.client.files.delete(fid)
                except Exception as e:
                    print(f"  Warning: Failed to delete file {fid}: {e}")

        latency_ms = (time.time() - start) * 1000

        if content_filter_triggered:
            return LLMResponse(
                raw_text="",
                parsed_json=None,
                model=self.model,
                input_tokens=0,
                output_tokens=0,
                latency_ms=latency_ms,
                stop_reason="content_filter",
                output_files=None,
            )

        assert response is not None
        response_text = response.output_text or ""

        # Extract files from container - list all files and filter out uploaded inputs
        output_files: list[OutputFile] = []
        container_id = None
        if hasattr(response, "output") and response.output:
            for item in response.output:
                if getattr(item, "type", None) == "code_interpreter_call":
                    container_id = getattr(item, "container_id", None)
                    if container_id:
                        break

        if container_id:
            try:
                container_files = self.client.containers.files.list(
                    container_id=container_id
                )
                uploaded_names = {f.name for f in (input_files or [])}
                for idx, cf in enumerate(container_files.data):
                    fid = getattr(cf, "id", None)
                    fpath = getattr(cf, "path", None) or f"output_{idx + 1}.bin"
                    fname = fpath.split("/")[-1] if "/" in fpath else fpath
                    is_uploaded = any(uname in fname for uname in uploaded_names)
                    if fid and not is_uploaded:
                        print(f"  Downloading output file: {fname}")
                        file_content = self.client.containers.files.content.retrieve(
                            fid, container_id=container_id
                        )
                        output_files.append(
                            OutputFile(
                                filename=fname,
                                content=cast(bytes, _read_file_content(file_content)),
                                mime_type="application/octet-stream",
                            )
                        )
            except Exception as e:
                print(f"  Warning: Failed to retrieve container files: {e}")

        usage = response.usage
        input_tokens = usage.input_tokens if usage else 0
        output_tokens = usage.output_tokens if usage else 0

        stop_reason = getattr(response, "stop_reason", None) or "unknown"
        if stop_reason == "length":
            stop_reason = "max_tokens"
            print("  WARNING: Output truncated (hit max_tokens limit)")

        parsed_json = extract_json(response_text)

        return LLMResponse(
            raw_text=response_text.strip(),
            parsed_json=parsed_json,
            model=self.model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            latency_ms=latency_ms,
            stop_reason=stop_reason,
            output_files=output_files if output_files else None,
        )


class GeminiRunner:
    """Run tasks against Google Gemini models using Files API + Code Execution."""

    def __init__(self, api_key: str | None = None, model: str | None = None):
        if not model:
            raise ValueError("model is required for GeminiRunner")
        self.api_key = (
            api_key
            or os.environ.get("GEMINI_API_KEY")
            or os.environ.get("GOOGLE_API_KEY")
        )
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY or GOOGLE_API_KEY not set")
        self.model = model
        self._client = None

    @property
    def client(self):
        if self._client is None:
            from google import genai

            self._client = genai.Client(api_key=self.api_key)
        return self._client

    def _upload_file(self, path: Path) -> object:
        """Upload file to Gemini Files API."""
        print(f"Uploading {path.name} to Gemini Files API...")
        return self.client.files.upload(file=str(path))

    @retry_on_rate_limit(max_retries=3, initial_wait=60)
    def run(self, task: Task, input_files: list[Path] | None = None) -> LLMResponse:
        """Execute a task using Gemini with file upload and code execution."""
        from google.genai import types

        start = time.time()

        uploaded_files = []
        files_to_upload = input_files or []
        for f in files_to_upload:
            if f and f.exists():
                uploaded_file = self._upload_file(f)
                uploaded_files.append(uploaded_file)

        contents = uploaded_files + [task.prompt]

        config = types.GenerateContentConfig(
            tools=[types.Tool(code_execution=types.ToolCodeExecution())],
            temperature=0,
            max_output_tokens=16384,
        )

        print("  Running Gemini model...")
        content_filter_triggered = False
        try:
            response = self.client.models.generate_content(
                model=self.model,
                contents=contents,
                config=config,
            )
        except Exception as e:
            if _is_content_filter_error(str(e)):
                print("  BLOCKED: Content filter triggered")
                content_filter_triggered = True
                response = None
            else:
                raise
        finally:
            for uploaded_file in uploaded_files:
                try:
                    self.client.files.delete(name=uploaded_file.name)
                except Exception as e:
                    print(f"  Warning: Failed to delete file {uploaded_file.name}: {e}")

        latency_ms = (time.time() - start) * 1000

        if content_filter_triggered:
            return LLMResponse(
                raw_text="",
                parsed_json=None,
                model=self.model,
                input_tokens=0,
                output_tokens=0,
                latency_ms=latency_ms,
                stop_reason="content_filter",
                output_files=None,
            )

        assert response is not None
        if response.candidates:
            finish_reason = getattr(response.candidates[0], "finish_reason", None)
            if finish_reason and "safety" in str(finish_reason).lower():
                print(
                    f"  BLOCKED: Content filter triggered (finish_reason={finish_reason})"
                )
                usage = response.usage_metadata
                prompt_tokens = usage.prompt_token_count if usage else None
                return LLMResponse(
                    raw_text="",
                    parsed_json=None,
                    model=self.model,
                    input_tokens=prompt_tokens or 0,
                    output_tokens=0,
                    latency_ms=latency_ms,
                    stop_reason="content_filter",
                    output_files=None,
                )

        response_text = ""
        output_files = []
        file_counter = 0

        content = response.candidates[0].content if response.candidates else None
        parts = content.parts if content else []
        if parts:
            for part in parts:
                if hasattr(part, "text") and part.text is not None:
                    response_text += part.text + "\n"
                if hasattr(part, "inline_data") and part.inline_data:
                    file_counter += 1
                    mime_type = getattr(
                        part.inline_data, "mime_type", "application/octet-stream"
                    )
                    ext = mime_type.split("/")[-1] if "/" in mime_type else "bin"
                    if ext == "vnd.openxmlformats-officedocument.spreadsheetml.sheet":
                        ext = "xlsx"
                    filename = f"output_{file_counter}.{ext}"
                    print(f"  Found output file: {filename} ({mime_type})")
                    inline_data = getattr(part.inline_data, "data", None)
                    output_files.append(
                        OutputFile(
                            filename=filename,
                            content=inline_data or b"",
                            mime_type=mime_type,
                        )
                    )

        usage = response.usage_metadata
        input_tokens = (usage.prompt_token_count if usage else None) or 0
        output_tokens = (usage.candidates_token_count if usage else None) or 0

        stop_reason = "unknown"
        if response.candidates:
            finish_reason = getattr(response.candidates[0], "finish_reason", None)
            if finish_reason:
                stop_reason = str(finish_reason).lower().replace("finishreason.", "")
                if stop_reason == "max_tokens":
                    print("  WARNING: Output truncated (hit max_tokens limit)")

        parsed_json = extract_json(response_text)

        return LLMResponse(
            raw_text=response_text.strip(),
            parsed_json=parsed_json,
            model=self.model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            latency_ms=latency_ms,
            stop_reason=stop_reason,
            output_files=output_files if output_files else None,
        )


# -----------------------------------------------------------------------------
# Azure Agent Runner - Pure Functions (Functional Core)
# -----------------------------------------------------------------------------


def _categorize_input_files(
    files: list[Path],
) -> tuple[list[Path], list[Path]]:
    """
    Categorize input files by the tool they require.

    :param files: List of input file paths
    :returns: (code_interpreter_files, file_search_files)

    Code interpreter: xlsx, xls, csv, png, jpg, jpeg
    File search: pdf
    """
    code_interpreter_exts = {".xlsx", ".xls", ".csv", ".png", ".jpg", ".jpeg"}
    file_search_exts = {".pdf"}

    code_files = []
    search_files = []

    for f in files:
        ext = f.suffix.lower()
        if ext in code_interpreter_exts:
            code_files.append(f)
        elif ext in file_search_exts:
            search_files.append(f)

    return code_files, search_files


def _extract_text_from_messages(messages: list) -> str:
    """
    Extract text content from Azure agent messages.

    :param messages: List of agent message objects
    :returns: Concatenated text from assistant messages

    Handles: text_messages attribute on message objects.
    Does not handle: Complex nested content structures.
    """
    text_parts = []

    for msg in messages:
        if getattr(msg, "role", None) != "assistant":
            continue

        # Handle text_messages attribute (list of text message objects)
        text_messages = getattr(msg, "text_messages", None)
        if text_messages:
            for tm in text_messages:
                text_value = getattr(tm, "text", None)
                if text_value:
                    # text might be an object with .value or a string
                    if hasattr(text_value, "value"):
                        text_parts.append(text_value.value)
                    elif isinstance(text_value, str):
                        text_parts.append(text_value)

        # Handle content attribute (list of content blocks)
        content = getattr(msg, "content", None)
        if content and isinstance(content, list):
            for block in content:
                if hasattr(block, "text"):
                    text_obj = block.text
                    if hasattr(text_obj, "value"):
                        text_parts.append(text_obj.value)
                    elif isinstance(text_obj, str):
                        text_parts.append(text_obj)

    return "\n".join(text_parts)


def _map_run_status_to_stop_reason(status: str, last_error: str | None) -> str:
    """
    Map Azure run status to standardized stop_reason.

    :param status: Azure run status (completed, failed, expired, etc.)
    :param last_error: Error message if run failed
    :returns: Normalized stop_reason string
    """
    status_lower = status.lower() if status else "unknown"

    if status_lower == "completed":
        return "end_turn"
    elif status_lower == "failed":
        if last_error and _is_content_filter_error(last_error):
            return "content_filter"
        return "failed"
    elif status_lower == "expired":
        return "expired"
    elif status_lower == "cancelled":
        return "cancelled"
    else:
        return status_lower


# -----------------------------------------------------------------------------
# Azure Agent Runner - Class with Side Effects (Imperative Shell)
# -----------------------------------------------------------------------------


class AzureAgentRunner:
    """
    Run tasks against Azure AI Foundry Agent Service.

    Supports multiple model providers (OpenAI, DeepSeek, Llama, Grok, etc.)
    via the unified Agent Service API. Uses code_interpreter for Excel/CSV
    and file_search for PDFs.

    :param api_key: Azure API key (optional if using DefaultAzureCredential)
    :param model: Deployment name in Azure AI Foundry (required)

    Environment variables:
        AZURE_AI_PROJECT_ENDPOINT: Project endpoint URL
        AZURE_AI_PROJECT_CONNECTION_STRING: Alternative to endpoint
    """

    def __init__(self, api_key: str | None = None, model: str | None = None):
        if not model:
            raise ValueError("model (deployment name) is required for AzureAgentRunner")

        self.model = model
        self.api_key = api_key
        self._client = None

        self._endpoint = os.environ.get("AZURE_AI_PROJECT_ENDPOINT")
        self._connection_string = os.environ.get("AZURE_AI_PROJECT_CONNECTION_STRING")
        self._bing_connection_id = os.environ.get("AZURE_BING_CONNECTION_ID")

        if not self._endpoint and not self._connection_string:
            raise ValueError(
                "AZURE_AI_PROJECT_ENDPOINT or AZURE_AI_PROJECT_CONNECTION_STRING must be set"
            )

    @property
    def client(self):
        """Lazy initialization of Azure AI Project client."""
        if self._client is None:
            from azure.ai.projects import AIProjectClient
            from azure.identity import DefaultAzureCredential

            if self._connection_string:
                self._client = AIProjectClient.from_connection_string(
                    credential=DefaultAzureCredential(),
                    conn_str=self._connection_string,
                )
            else:
                self._client = AIProjectClient(
                    endpoint=self._endpoint,
                    credential=DefaultAzureCredential(),
                )
        return self._client

    def _upload_file(self, path: Path, purpose: str) -> str:
        """
        Upload a file to Azure for use with agents.

        :param path: Local file path
        :param purpose: File purpose (e.g., "agents")
        :returns: Uploaded file ID
        """
        from azure.ai.agents.models import FilePurpose

        purpose_enum = FilePurpose.AGENTS if purpose == "agents" else FilePurpose.AGENTS
        print(f"  Uploading {path.name} to Azure...")
        file = self.client.agents.files.upload_and_poll(
            file_path=str(path), purpose=purpose_enum
        )
        return file.id

    def _create_vector_store(self, file_ids: list[str], name: str) -> str:
        """
        Create a vector store for file search.

        :param file_ids: List of uploaded file IDs
        :param name: Vector store name
        :returns: Vector store ID
        """
        print("  Creating vector store for file search...")
        vector_store = self.client.agents.vector_stores.create_and_poll(
            file_ids=file_ids, name=name
        )
        return vector_store.id

    def _delete_file(self, file_id: str) -> None:
        """Delete an uploaded file."""
        try:
            self.client.agents.files.delete(file_id)
        except Exception as e:
            print(f"  Warning: Failed to delete file {file_id}: {e}")

    def _delete_vector_store(self, vector_store_id: str) -> None:
        """Delete a vector store."""
        try:
            self.client.agents.vector_stores.delete(vector_store_id)
        except Exception as e:
            print(f"  Warning: Failed to delete vector store {vector_store_id}: {e}")

    def _delete_agent(self, agent_id: str) -> None:
        """Delete an agent."""
        try:
            self.client.agents.delete_agent(agent_id)
        except Exception as e:
            print(f"  Warning: Failed to delete agent {agent_id}: {e}")

    def _download_output_files(self, messages: list) -> list[OutputFile]:
        """
        Download files generated by the agent (charts, modified Excel, etc.).

        :param messages: List of agent messages
        :returns: List of OutputFile objects
        """
        output_files = []
        file_counter = 0

        for msg in messages:
            if getattr(msg, "role", None) != "assistant":
                continue

            # Check for image_contents
            image_contents = getattr(msg, "image_contents", None)
            if image_contents:
                for img in image_contents:
                    image_file = getattr(img, "image_file", None)
                    if image_file:
                        file_id = getattr(image_file, "file_id", None)
                        if file_id:
                            file_counter += 1
                            filename = f"output_{file_counter}.png"
                            try:
                                print(f"  Downloading output file: {filename}")
                                content = b"".join(
                                    self.client.agents.files.get_content(file_id)
                                )
                                output_files.append(
                                    OutputFile(
                                        filename=filename,
                                        content=content,
                                        mime_type="image/png",
                                    )
                                )
                            except Exception as e:
                                print(f"  Warning: Failed to download {file_id}: {e}")

            text_messages = getattr(msg, "text_messages", None)
            if text_messages:
                for tm in text_messages:
                    text_obj = getattr(tm, "text", None)
                    annotations = (
                        getattr(text_obj, "annotations", None) if text_obj else None
                    )
                    if annotations:
                        for ann in annotations:
                            file_path = (
                                ann.get("file_path")
                                if isinstance(ann, dict)
                                else getattr(ann, "file_path", None)
                            )
                            if file_path:
                                file_id = (
                                    file_path.get("file_id")
                                    if isinstance(file_path, dict)
                                    else getattr(file_path, "file_id", None)
                                )
                                if file_id:
                                    file_counter += 1
                                    ext = "xlsx"
                                    filename = f"output_{file_counter}.{ext}"
                                    try:
                                        print(f"  Downloading output file: {filename}")
                                        content = b"".join(
                                            self.client.agents.files.get_content(
                                                file_id
                                            )
                                        )
                                        output_files.append(
                                            OutputFile(
                                                filename=filename,
                                                content=content,
                                                mime_type="application/octet-stream",
                                            )
                                        )
                                    except Exception as e:
                                        print(
                                            f"  Warning: Failed to download {file_id}: {e}"
                                        )
                    if annotations:
                        for ann in annotations:
                            file_path = getattr(ann, "file_path", None)
                            if file_path:
                                file_id = getattr(file_path, "file_id", None)
                                if file_id:
                                    file_counter += 1
                                    ext = "xlsx"  # Default for file outputs
                                    filename = f"output_{file_counter}.{ext}"
                                    try:
                                        print(f"  Downloading output file: {filename}")
                                        content = b"".join(
                                            self.client.agents.files.get_content(
                                                file_id
                                            )
                                        )
                                        output_files.append(
                                            OutputFile(
                                                filename=filename,
                                                content=content,
                                                mime_type="application/octet-stream",
                                            )
                                        )
                                    except Exception as e:
                                        print(
                                            f"  Warning: Failed to download {file_id}: {e}"
                                        )

        return output_files

    @retry_on_rate_limit(max_retries=3, initial_wait=60)
    def run(self, task: Task, input_files: list[Path] | None = None) -> LLMResponse:
        """
        Execute a task using Azure AI Foundry Agent Service.

        :param task: Task object with prompt and metadata
        :param input_files: Optional list of input file paths
        :returns: LLMResponse with text, parsed JSON, and output files

        Uses code_interpreter for xlsx/csv files, file_search for PDFs.
        Agent and resources are cleaned up after execution.
        """
        from azure.ai.agents.models import (
            BingGroundingTool,
            CodeInterpreterTool,
            FileSearchTool,
            ToolResources,
        )

        start = time.time()
        files = input_files or []

        # Pure: categorize files
        code_files, search_files = _categorize_input_files(files)

        # Track resources for cleanup
        uploaded_file_ids: list[str] = []
        vector_store_id: str | None = None
        agent_id: str | None = None

        try:
            # Upload files for code interpreter
            code_file_ids = []
            for f in code_files:
                fid = self._upload_file(f, "agents")
                code_file_ids.append(fid)
                uploaded_file_ids.append(fid)

            # Upload files for file search and create vector store
            search_file_ids = []
            for f in search_files:
                fid = self._upload_file(f, "agents")
                search_file_ids.append(fid)
                uploaded_file_ids.append(fid)

            if search_file_ids:
                vector_store_id = self._create_vector_store(
                    search_file_ids, "ib-bench-docs"
                )

            # Build tools and tool_resources
            tools = []
            code_interpreter_resource = None
            file_search_resource = None

            if code_file_ids:
                code_interpreter = CodeInterpreterTool(file_ids=code_file_ids)
                tools.extend(code_interpreter.definitions)
                code_interpreter_resource = code_interpreter.resources.code_interpreter
            elif files:
                # Enable code interpreter even without files (for analysis)
                code_interpreter = CodeInterpreterTool()
                tools.extend(code_interpreter.definitions)

            if vector_store_id:
                file_search = FileSearchTool(vector_store_ids=[vector_store_id])
                tools.extend(file_search.definitions)
                file_search_resource = file_search.resources.file_search

            if self._bing_connection_id:
                bing = BingGroundingTool(connection_id=self._bing_connection_id)
                tools.extend(bing.definitions)

            # Combine tool resources
            tool_resources = None
            if code_interpreter_resource or file_search_resource:
                tool_resources = ToolResources(
                    code_interpreter=code_interpreter_resource,
                    file_search=file_search_resource,
                )

            # Create agent
            print(f"  Creating agent with model {self.model}...")
            agent = self.client.agents.create_agent(
                model=self.model,
                name="ib-bench-agent",
                instructions="You are an expert investment banking analyst. Analyze the provided files and respond precisely to the task.",
                tools=tools if tools else None,
                tool_resources=tool_resources if tool_resources else None,
                temperature=0,
            )
            agent_id = agent.id

            # Create thread
            thread = self.client.agents.threads.create()

            # Add message
            self.client.agents.messages.create(
                thread_id=thread.id, role="user", content=task.prompt
            )

            # Run agent
            print("  Running agent...")
            run = self.client.agents.runs.create_and_process(
                thread_id=thread.id, agent_id=agent_id
            )

            # Get run result
            run_status = getattr(run, "status", "unknown")
            last_error = None
            if run_status == "failed":
                last_error_obj = getattr(run, "last_error", None)
                last_error = str(last_error_obj) if last_error_obj else None
                print(f"  Run failed: {last_error}")

            # Get messages
            messages = list(self.client.agents.messages.list(thread_id=thread.id))

            # Get usage (may not be available on all runs)
            usage = getattr(run, "usage", None)
            input_tokens = getattr(usage, "prompt_tokens", 0) if usage else 0
            output_tokens = getattr(usage, "completion_tokens", 0) if usage else 0

            latency_ms = (time.time() - start) * 1000

            # Pure: extract text from messages
            raw_text = _extract_text_from_messages(messages)

            # Pure: map status to stop_reason
            stop_reason = _map_run_status_to_stop_reason(run_status, last_error)

            if stop_reason == "content_filter":
                print("  BLOCKED: Content filter triggered")
                return LLMResponse(
                    raw_text="",
                    parsed_json=None,
                    model=self.model,
                    input_tokens=input_tokens,
                    output_tokens=output_tokens,
                    latency_ms=latency_ms,
                    stop_reason="content_filter",
                    output_files=None,
                )

            # Download output files
            output_files = self._download_output_files(messages)

            # Pure: parse JSON from response
            parsed_json = extract_json(raw_text)

            return LLMResponse(
                raw_text=raw_text.strip(),
                parsed_json=parsed_json,
                model=self.model,
                input_tokens=input_tokens,
                output_tokens=output_tokens,
                latency_ms=latency_ms,
                stop_reason=stop_reason,
                output_files=output_files if output_files else None,
            )

        finally:
            # Cleanup resources
            if agent_id:
                self._delete_agent(agent_id)
            if vector_store_id:
                self._delete_vector_store(vector_store_id)
            for fid in uploaded_file_ids:
                self._delete_file(fid)
