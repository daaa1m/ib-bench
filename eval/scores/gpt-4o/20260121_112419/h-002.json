{
  "task_id": "h-002",
  "rubric_hash": "6d655c7d",
  "scored_at": "2026-01-21T12:43:07.097836",
  "passed": false,
  "total_points": 100,
  "points_earned": 9.5,
  "score_percent": 9.5,
  "llm_gated": false,
  "criteria": [
    {
      "id": "historical_updates",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 20,
      "points_earned": 0,
      "actual": "0.00",
      "details": "Human score: 0.00 - Complete hallucination",
      "score": 0,
      "reasoning": "Complete hallucination",
      "description": "Historical inputs are populated correctly for the last three fiscal years with credible sourcing",
      "scoring_guide": "1.0 = correct and complete historicals with sources; 0.5 = partial or minor errors; 0 = missing/incorrect"
    },
    {
      "id": "assumptions_summary",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 15,
      "points_earned": 0,
      "actual": "0.00",
      "details": "Human score: 0.00 - Unreasonable",
      "score": 0,
      "reasoning": "Unreasonable",
      "description": "Entry, leverage, and projection assumptions are reasonable and internally consistent",
      "scoring_guide": "1.0 = clear, defensible assumptions; 0.5 = partially reasonable; 0 = unrealistic or missing"
    },
    {
      "id": "returns_summary",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 15,
      "points_earned": 7.5,
      "actual": "0.50",
      "details": "Human score: 0.50 - Returns are coherent, but hardcoded",
      "score": 0.5,
      "reasoning": "Returns are coherent, but hardcoded",
      "description": "Returns analysis (IRR/MOIC) and exit assumptions are coherent and supported by the model",
      "scoring_guide": "1.0 = returns computed and consistent; 0.5 = partial or inconsistent; 0 = missing/incorrect"
    },
    {
      "id": "business_assessment",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 25,
      "points_earned": 0,
      "actual": "0.00",
      "details": "Human score: 0.00 - Did not actually do an assessment, simply copied example",
      "score": 0,
      "reasoning": "Did not actually do an assessment, simply copied example",
      "description": "Business assessment covers market position, strengths/weaknesses, risks, and competitive landscape",
      "scoring_guide": "1.0 = thorough, specific assessment; 0.5 = surface-level; 0 = missing/irrelevant"
    },
    {
      "id": "recommendation",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 10,
      "points_earned": 1.0,
      "actual": "0.10",
      "details": "Human score: 0.10 - Incoherent, suggested Go but also do not invest",
      "score": 0.1,
      "reasoning": "Incoherent, suggested Go but also do not invest",
      "description": "Go/No-Go recommendation is clear and supported by rationale and next steps",
      "scoring_guide": "1.0 = clear decision with rationale; 0.5 = vague; 0 = missing"
    },
    {
      "id": "model_integrity_checks",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 10,
      "points_earned": 1.0,
      "actual": "0.10",
      "details": "Human score: 0.10 - Hardcoded values everywhere but somewhat coherent",
      "score": 0.1,
      "reasoning": "Hardcoded values everywhere but somewhat coherent",
      "description": "Model ties and reconciliation checks are validated after updates",
      "scoring_guide": "1.0 = checks explicitly confirmed; 0.5 = partial checks; 0 = no validation"
    },
    {
      "id": "formatting",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 5,
      "points_earned": 0,
      "actual": "0.00",
      "details": "Human score: 0.00 - Non existent",
      "score": 0,
      "reasoning": "Non existent",
      "description": "IB formatting conventions and existing model style are followed for modified cells",
      "scoring_guide": "1.0 = formatting consistent and correct; 0.5 = minor issues; 0 = incorrect formatting"
    }
  ],
  "judge": "human",
  "response_file": "/Users/da1m/Code/ib-bench/eval/responses/gpt-4o/20260121_112419/h-002.json",
  "human_template": "/Users/da1m/Code/ib-bench/eval/scores/gpt-4o/20260121_112419/h-002.human.md"
}