{
  "task_id": "h-002",
  "rubric_hash": "6d655c7d",
  "scored_at": "2026-01-14T12:24:48.012821",
  "passed": true,
  "total_points": 100,
  "points_earned": 66.0,
  "score_percent": 66.0,
  "llm_gated": false,
  "criteria": [
    {
      "id": "historical_updates",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 20,
      "points_earned": 10.0,
      "actual": "0.50",
      "details": "Human score: 0.50 - Decent at gathering inputs across multiple sources. Accuracy not 100%",
      "score": 0.5,
      "reasoning": "Decent at gathering inputs across multiple sources. Accuracy not 100%",
      "description": "Historical inputs are populated correctly for the last three fiscal years with credible sourcing",
      "scoring_guide": "1.0 = correct and complete historicals with sources; 0.5 = partial or minor errors; 0 = missing/incorrect"
    },
    {
      "id": "assumptions_summary",
      "passed": true,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 15,
      "points_earned": 12.0,
      "actual": "0.80",
      "details": "Human score: 0.80 - Reasonable assumptions",
      "score": 0.8,
      "reasoning": "Reasonable assumptions",
      "description": "Entry, leverage, and projection assumptions are reasonable and internally consistent",
      "scoring_guide": "1.0 = clear, defensible assumptions; 0.5 = partially reasonable; 0 = unrealistic or missing"
    },
    {
      "id": "returns_summary",
      "passed": true,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 15,
      "points_earned": 9.0,
      "actual": "0.60",
      "details": "Human score: 0.60 - Good ideas",
      "score": 0.6,
      "reasoning": "Good ideas",
      "description": "Returns analysis (IRR/MOIC) and exit assumptions are coherent and supported by the model",
      "scoring_guide": "1.0 = returns computed and consistent; 0.5 = partial or inconsistent; 0 = missing/incorrect"
    },
    {
      "id": "business_assessment",
      "passed": true,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 25,
      "points_earned": 20.0,
      "actual": "0.80",
      "details": "Human score: 0.80 - High quality assessment",
      "score": 0.8,
      "reasoning": "High quality assessment",
      "description": "Business assessment covers market position, strengths/weaknesses, risks, and competitive landscape",
      "scoring_guide": "1.0 = thorough, specific assessment; 0.5 = surface-level; 0 = missing/irrelevant"
    },
    {
      "id": "recommendation",
      "passed": true,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 10,
      "points_earned": 6.0,
      "actual": "0.60",
      "details": "Human score: 0.60 - Vague and non committal",
      "score": 0.6,
      "reasoning": "Vague and non committal",
      "description": "Go/No-Go recommendation is clear and supported by rationale and next steps",
      "scoring_guide": "1.0 = clear decision with rationale; 0.5 = vague; 0 = missing"
    },
    {
      "id": "model_integrity_checks",
      "passed": true,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 10,
      "points_earned": 7.0,
      "actual": "0.70",
      "details": "Human score: 0.70 - Did not break model",
      "score": 0.7,
      "reasoning": "Did not break model",
      "description": "Model ties and reconciliation checks are validated after updates",
      "scoring_guide": "1.0 = checks explicitly confirmed; 0.5 = partial checks; 0 = no validation"
    },
    {
      "id": "formatting",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 5,
      "points_earned": 2.0,
      "actual": "0.40",
      "details": "Human score: 0.40 - Could be nicer",
      "score": 0.4,
      "reasoning": "Could be nicer",
      "description": "IB formatting conventions and existing model style are followed for modified cells",
      "scoring_guide": "1.0 = formatting consistent and correct; 0.5 = minor issues; 0 = incorrect formatting"
    }
  ],
  "judge": "human",
  "response_file": "/Users/da1m/Code/ib-bench/eval/responses/claude-opus-4-5-20251101/20260114_072507/h-002.json",
  "human_template": "/Users/da1m/Code/ib-bench/eval/scores/claude-opus-4-5-20251101/20260114_072507/h-002.human.md"
}