{
  "task_id": "h-001",
  "rubric_hash": "d84b33a7",
  "scored_at": "2026-01-13T22:54:12.392006",
  "passed": false,
  "total_points": 100,
  "points_earned": 0,
  "score_percent": 0.0,
  "llm_gated": false,
  "criteria": [
    {
      "id": "quarterly_updates",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 20,
      "points_earned": 0,
      "actual": "[PENDING]",
      "details": "Human scoring required (rubric specifies human judge)",
      "score": 0,
      "reasoning": "Looks almos correct but actually unusable",
      "description": "Quarterly sheet is updated for the latest three years with correct values and segmentation",
      "scoring_guide": "1.0 = all relevant quarterly rows updated correctly; 0.5 = partial or minor errors; 0 = missing/incorrect updates"
    },
    {
      "id": "revbuild_segmentation_updates",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 25,
      "points_earned": 0,
      "actual": "[PENDING]",
      "details": "Human scoring required (rubric specifies human judge)",
      "score": null,
      "reasoning": "",
      "description": "RevBuild Calcs reflects the new revenue segmentation and feeds the model correctly",
      "scoring_guide": "1.0 = segmentation change correctly implemented and flows through; 0.5 = partial mapping or linkage issues; 0 = not updated"
    },
    {
      "id": "financial_statements_updates",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 25,
      "points_earned": 0,
      "actual": "[PENDING]",
      "details": "Human scoring required (rubric specifies human judge)",
      "score": null,
      "reasoning": "",
      "description": "P&L and BS & CFS (including FS/Debt calcs) are updated and consistent with the new segmentation",
      "scoring_guide": "1.0 = statements updated and internally consistent; 0.5 = partial updates or inconsistencies; 0 = missing/incorrect"
    },
    {
      "id": "output_summary_updates",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 15,
      "points_earned": 0,
      "actual": "[PENDING]",
      "details": "Human scoring required (rubric specifies human judge)",
      "score": null,
      "reasoning": "",
      "description": "Output and Summary sheets reflect the updated model results",
      "scoring_guide": "1.0 = output/summary refreshed correctly; 0.5 = partial refresh; 0 = not updated"
    },
    {
      "id": "model_integrity_checks",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 10,
      "points_earned": 0,
      "actual": "[PENDING]",
      "details": "Human scoring required (rubric specifies human judge)",
      "score": null,
      "reasoning": "",
      "description": "Model ties and reconciliation checks are validated after updates",
      "scoring_guide": "1.0 = ties/reconciliations explicitly confirmed; 0.5 = partial checks; 0 = no validation"
    },
    {
      "id": "formatting",
      "passed": false,
      "type": "human_judge",
      "match_type": "human_judge",
      "points": 5,
      "points_earned": 0,
      "actual": "[PENDING]",
      "details": "Human scoring required (rubric specifies human judge)",
      "score": null,
      "reasoning": "",
      "description": "IB formatting conventions and existing model style are followed for modified cells",
      "scoring_guide": "1.0 = formatting consistent and correct; 0.5 = minor issues; 0 = incorrect formatting"
    }
  ],
  "judge": "human-pending",
  "response_file": "/Users/da1m/Code/ib-bench/eval/responses/claude-opus-4-5-20251101/20260113_215424/h-001.json",
  "human_template": "/Users/da1m/Code/ib-bench/eval/scores/claude-opus-4-5-20251101/20260113_215424/h-001.human.md"
}

