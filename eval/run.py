"""
Run evaluation on tasks.

Usage:
    uv run python eval/run.py --config configs/quick-test.yaml
    uv run python eval/run.py --config configs/full-easy.yaml
    uv run python eval/run.py --config configs/quick-test.yaml --resume MODEL/RUN_ID
"""

import argparse
import asyncio
import json
from datetime import datetime
from pathlib import Path

import yaml

from helpers import (
    load_tasks,
    get_runner,
    create_run_directory,
    Task,
    AnthropicRunner,
    OpenAIRunner,
    GeminiRunner,
)

# Type alias for any runner
Runner = AnthropicRunner | OpenAIRunner | GeminiRunner


def load_config(config_path: Path) -> dict:
    """Load configuration from YAML file."""
    with open(config_path) as f:
        return yaml.safe_load(f) or {}


def run_task(task: Task, runner: Runner, run_dir: Path) -> dict:
    """Execute a single task and save the response."""
    print(f"Running task {task.id}...")

    # Find input files (xlsx, pdf, etc.)
    input_files = [
        f for f in task.input_files if f.suffix.lower() in [".xlsx", ".pdf", ".xls"]
    ]

    if input_files:
        print(f"  Input files: {[f.name for f in input_files]}")

    response = runner.run(task, input_files=input_files)

    # Save any output files generated by the model
    output_file_paths = []
    if response.output_files:
        for i, out_file in enumerate(response.output_files):
            # Use original filename or generate one
            ext = out_file.filename.split('.')[-1] if '.' in out_file.filename else 'bin'
            output_path = run_dir / f"{task.id}_output_{i+1}.{ext}"
            with open(output_path, "wb") as f:
                f.write(out_file.content)
            output_file_paths.append(output_path.name)
            print(f"  Saved output file: {output_path.name}")

    # Prepare response data
    response_data = {
        "task_id": task.id,
        "model": response.model,
        "timestamp": datetime.now().isoformat(),
        "input_files": [f.name for f in input_files],
        "output_files": output_file_paths,
        "raw_response": response.raw_text,
        "parsed_response": response.parsed_json,
        "stop_reason": response.stop_reason,
        "usage": {
            "input_tokens": response.input_tokens,
            "output_tokens": response.output_tokens,
            "latency_ms": response.latency_ms,
        },
    }

    # Save response, and dump it as json e.g., e-001.json
    # run_dir => ...{model}/{timestamp}
    response_path = run_dir / f"{task.id}.json"
    with open(response_path, "w") as f:
        json.dump(response_data, f, indent=2)

    print(f"  Saved response to {response_path}")
    print(f"  Tokens: {response.input_tokens} in / {response.output_tokens} out")
    print(f"  Latency: {response.latency_ms:.0f}ms")

    return response_data


async def run_task_async(
    task: Task, runner: Runner, run_dir: Path, semaphore: asyncio.Semaphore
) -> dict:
    """Execute a single task asynchronously with rate limiting."""
    async with semaphore:
        print(f"Running task {task.id}...")

        input_files = [
            f for f in task.input_files if f.suffix.lower() in [".xlsx", ".pdf", ".xls"]
        ]

        # Run sync runner in thread pool
        response = await asyncio.to_thread(runner.run, task, input_files)

        # Save any output files generated by the model
        output_file_paths = []
        if response.output_files:
            for i, out_file in enumerate(response.output_files):
                ext = out_file.filename.split('.')[-1] if '.' in out_file.filename else 'bin'
                output_path = run_dir / f"{task.id}_output_{i+1}.{ext}"
                with open(output_path, "wb") as f:
                    f.write(out_file.content)
                output_file_paths.append(output_path.name)
                print(f"  Saved output file: {output_path.name}")

        # Prepare and save response
        response_data = {
            "task_id": task.id,
            "model": response.model,
            "timestamp": datetime.now().isoformat(),
            "input_files": [f.name for f in input_files],
            "output_files": output_file_paths,
            "raw_response": response.raw_text,
            "parsed_response": response.parsed_json,
            "stop_reason": response.stop_reason,
            "usage": {
                "input_tokens": response.input_tokens,
                "output_tokens": response.output_tokens,
                "latency_ms": response.latency_ms,
            },
        }

        response_path = run_dir / f"{task.id}.json"
        with open(response_path, "w") as f:
            json.dump(response_data, f, indent=2)

        print(
            f"  {task.id}: {response.input_tokens} in / {response.output_tokens} out ({response.latency_ms:.0f}ms)"
        )
        return {"task_id": task.id, "status": "success"}


async def run_tasks_parallel(
    tasks: list[Task], runner: Runner, run_dir: Path, max_concurrent: int
) -> list:
    """Run multiple tasks concurrently with rate limiting."""
    semaphore = asyncio.Semaphore(max_concurrent)

    async def safe_run(task):
        try:
            return await run_task_async(task, runner, run_dir, semaphore)
        except Exception as e:
            print(f"  ERROR {task.id}: {e}")
            return {"task_id": task.id, "status": "error", "error": str(e)}

    results = await asyncio.gather(*[safe_run(t) for t in tasks])
    return list(results)


def main():
    parser = argparse.ArgumentParser(
        description="Run IB-bench evaluation",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s --config configs/quick-test.yaml
  %(prog)s --config configs/full-easy.yaml
  %(prog)s --config configs/quick-test.yaml --resume MODEL/RUN_ID
        """,
    )
    parser.add_argument(
        "--config",
        type=Path,
        required=True,
        help="Path to YAML config file (e.g., configs/quick-test.yaml)",
    )
    parser.add_argument("--resume", help="Run ID to resume (e.g., MODEL/RUN_ID)")
    args = parser.parse_args()

    # Load config file
    config_path = Path(__file__).parent / args.config
    if not config_path.exists():
        config_path = args.config  # Try absolute path
    if not config_path.exists():
        parser.error(f"Config file not found: {args.config}")

    config = load_config(config_path)
    print(f"Loaded config from: {config_path}")

    # Extract config values
    provider: str | None = config.get("provider")
    model: str | None = config.get("model")
    task_ids: list[str] | None = config.get("tasks")
    filter_pattern: str | None = config.get("filter")
    parallel: int = config.get("parallel", 1)

    # Validate required fields
    if not provider:
        parser.error("Config must specify 'provider'")
    if not model:
        parser.error("Config must specify 'model'")
    if not task_ids and not filter_pattern:
        parser.error("Config must specify 'tasks' or 'filter'")

    # Load tasks (no rubric needed for running, only for scoring)
    tasks = load_tasks(
        task_ids=task_ids, filter_pattern=filter_pattern, include_rubric=False
    )
    if not tasks:
        print("No tasks found!")
        print(f"Looked for task IDs: {task_ids}")
        return

    print(f"Found {len(tasks)} task(s) to run")

    # Initialize runner
    runner = get_runner(provider, model)
    model_name = runner.model

    # Create or resume run directory
    existing_results = []
    original_started_at = None
    if args.resume:
        run_dir = Path(__file__).parent / "responses" / args.resume
        if not run_dir.exists():
            print(f"Run directory not found: {run_dir}")
            return
        # Load existing config to preserve results_summary and started_at
        existing_config_path = run_dir / "config.json"
        if existing_config_path.exists():
            with open(existing_config_path) as f:
                existing_config = json.load(f)
                existing_results = existing_config.get("results_summary", [])
                original_started_at = existing_config.get("started_at")
    else:
        run_dir = create_run_directory(model_name)

    print(f"Run directory: {run_dir}")

    # Save config
    run_config = {
        "provider": provider,
        "model": model_name,
        "task_ids": [t.id for t in tasks],
        "parallel": parallel,
        "config_file": str(args.config),
        "started_at": original_started_at or datetime.now().isoformat(),
    }
    run_config_path = run_dir / "config.json"
    with open(run_config_path, "w") as f:
        json.dump(run_config, f, indent=2)

    # Filter out already-completed tasks if resuming
    tasks_to_run = []
    for task in tasks:
        response_path = run_dir / f"{task.id}.json"
        if response_path.exists() and args.resume:
            print(f"Skipping {task.id} (already completed)")
        else:
            tasks_to_run.append(task)

    # Early exit if nothing to run
    if not tasks_to_run:
        print("\nAll tasks already completed!")
        return

    # Run tasks (parallel or sequential)
    if parallel > 1 and len(tasks_to_run) > 1:
        print(f"Running {len(tasks_to_run)} task(s) with {parallel} concurrent...")
        results = asyncio.run(
            run_tasks_parallel(tasks_to_run, runner, run_dir, parallel)
        )
    else:
        print(f"Running {len(tasks_to_run)} task(s) sequentially...")
        results = []
        for task in tasks_to_run:
            try:
                run_task(task, runner, run_dir)
                results.append({"task_id": task.id, "status": "success"})
            except Exception as e:
                print(f"  ERROR: {e}")
                results.append({"task_id": task.id, "status": "error", "error": str(e)})

    # Clean up if no successful responses
    successful = [r for r in results if r["status"] == "success"]
    if not successful and not args.resume:
        import shutil

        print(f"\nNo successful responses. Cleaning up {run_dir}")
        shutil.rmtree(run_dir)
        return

    # Update config with completion info
    # Merge existing results with new results (existing first, then new)
    existing_task_ids = {r["task_id"] for r in existing_results}
    merged_results = existing_results + [
        r for r in results if r["task_id"] not in existing_task_ids
    ]

    run_config["completed_at"] = datetime.now().isoformat()
    run_config["results_summary"] = merged_results
    with open(run_config_path, "w") as f:
        json.dump(run_config, f, indent=2)

    print(f"\nRun complete! Results in: {run_dir}")
    print(f"Run ID: {run_dir.name}")


if __name__ == "__main__":
    main()
